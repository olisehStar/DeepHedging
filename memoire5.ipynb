{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383ce8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional, Tuple, Callable\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96676b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Hedging Engine pour Produits GMAB\n",
    "Version compl√®te et optimis√©e pour la finance de march√©\n",
    "Auteur: Deep Hedging Team\n",
    "Date: F√©vrier 2026\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# CONFIGURATION TENSORFLOW\n",
    "# =================================================================================\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seeds pour reproductibilit√©\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# =================================================================================\n",
    "# CONFIGURATION LOGGING\n",
    "# =================================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# =================================================================================\n",
    "# ENUMS ET DATACLASSES\n",
    "# =================================================================================\n",
    "class ModelType(Enum):\n",
    "    \"\"\"Types de mod√®les support√©s.\"\"\"\n",
    "    FFNN = \"FFNN\"\n",
    "    LSTM = \"LSTM\"\n",
    "    GRU = \"GRU\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration du mod√®le de deep hedging.\"\"\"\n",
    "    model_type: ModelType = ModelType.LSTM\n",
    "    n_assets: int = 6  # S + 5 ZC\n",
    "    n_features: int = 11  # 10 market features + 1 portfolio value\n",
    "    hidden_layers: int = 2\n",
    "    units: int = 64\n",
    "    learning_rate: float = 0.001\n",
    "    alpha_cvar: float = 0.99\n",
    "    lambda_cost: float = 0.001\n",
    "    scale_factor_zc: float = 5.0\n",
    "    dropout_rate: float = 0.2\n",
    "    recurrent_dropout: float = 0.1\n",
    "    batch_normalization: bool = True\n",
    "    gradient_clip_norm: float = 1.0\n",
    "    l2_regularization: float = 1e-3\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convertit la config en dictionnaire.\"\"\"\n",
    "        return {\n",
    "            'model_type': self.model_type.value,\n",
    "            **{k: v for k, v in asdict(self).items() if k != 'model_type'}\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict) -> 'ModelConfig':\n",
    "        \"\"\"Cr√©e une config depuis un dictionnaire.\"\"\"\n",
    "        config_dict = config_dict.copy()\n",
    "        if 'model_type' in config_dict:\n",
    "            config_dict['model_type'] = ModelType(config_dict['model_type'])\n",
    "        return cls(**config_dict)\n",
    "\n",
    "# =================================================================================\n",
    "# CLASSES UTILITAIRES\n",
    "# =================================================================================\n",
    "class DataNormalizer:\n",
    "    \"\"\"Normaliseur de donn√©es avec sauvegarde des param√®tres.\"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon: float = 1e-8, method: str = 'standard'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            epsilon: Petite valeur pour √©viter division par z√©ro\n",
    "            method: 'standard' (z-score) ou 'minmax'\n",
    "        \"\"\"\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.min_val = None\n",
    "        self.max_val = None\n",
    "        self.epsilon = epsilon\n",
    "        self.method = method\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, data: np.ndarray) -> 'DataNormalizer':\n",
    "        \"\"\"Calcule les statistiques de normalisation.\"\"\"\n",
    "        if data.ndim != 3:\n",
    "            raise ValueError(f\"Data doit avoir 3 dimensions (B, T, D), re√ßu {data.ndim}\")\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            self.mean = np.mean(data, axis=(0, 1))\n",
    "            self.std = np.std(data, axis=(0, 1)) + self.epsilon\n",
    "        elif self.method == 'minmax':\n",
    "            self.min_val = np.min(data, axis=(0, 1))\n",
    "            self.max_val = np.max(data, axis=(0, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"M√©thode inconnue: {self.method}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        logger.info(f\"Normalizer fitted - method: {self.method}\")\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalise les donn√©es.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Normalizer must be fitted before transform\")\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            return (data - self.mean) / self.std\n",
    "        else:  # minmax\n",
    "            return (data - self.min_val) / (self.max_val - self.min_val + self.epsilon)\n",
    "    \n",
    "    def inverse_transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"D√©s-normalise les donn√©es.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Normalizer must be fitted before inverse_transform\")\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        else:  # minmax\n",
    "            return data * (self.max_val - self.min_val + self.epsilon) + self.min_val\n",
    "    \n",
    "    def fit_transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fit et transform en une seule √©tape.\"\"\"\n",
    "        return self.fit(data).transform(data)\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde le normaliseur.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Cannot save unfitted normalizer\")\n",
    "        \n",
    "        save_dict = {\n",
    "            'epsilon': self.epsilon,\n",
    "            'method': self.method\n",
    "        }\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            save_dict['mean'] = self.mean\n",
    "            save_dict['std'] = self.std\n",
    "        else:\n",
    "            save_dict['min_val'] = self.min_val\n",
    "            save_dict['max_val'] = self.max_val\n",
    "        \n",
    "        np.savez(path, **save_dict)\n",
    "        logger.info(f\"Normalizer sauvegard√© dans {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'DataNormalizer':\n",
    "        \"\"\"Charge un normaliseur sauvegard√©.\"\"\"\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        method = str(data['method'])\n",
    "        epsilon = float(data['epsilon'])\n",
    "        \n",
    "        normalizer = cls(epsilon=epsilon, method=method)\n",
    "        \n",
    "        if method == 'standard':\n",
    "            normalizer.mean = data['mean']\n",
    "            normalizer.std = data['std']\n",
    "        else:\n",
    "            normalizer.min_val = data['min_val']\n",
    "            normalizer.max_val = data['max_val']\n",
    "        \n",
    "        normalizer.is_fitted = True\n",
    "        logger.info(f\"Normalizer charg√© depuis {path}\")\n",
    "        return normalizer\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Impl√©mentation am√©lior√©e d'early stopping.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 patience: int = 10, \n",
    "                 min_delta: float = 0.0, \n",
    "                 restore_best_weights: bool = True,\n",
    "                 mode: str = 'min'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Nombre d'√©poques sans am√©lioration avant arr√™t\n",
    "            min_delta: Am√©lioration minimale pour compter comme progr√®s\n",
    "            restore_best_weights: Si True, restaure les meilleurs poids\n",
    "            mode: 'min' pour minimiser, 'max' pour maximiser\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.mode = mode\n",
    "        self.best_loss = float('inf') if mode == 'min' else float('-inf')\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        self.stopped_epoch = 0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def _is_improvement(self, current_loss: float) -> bool:\n",
    "        \"\"\"V√©rifie si la perte actuelle est une am√©lioration.\"\"\"\n",
    "        if self.mode == 'min':\n",
    "            return current_loss < self.best_loss - self.min_delta\n",
    "        else:\n",
    "            return current_loss > self.best_loss + self.min_delta\n",
    "    \n",
    "    def __call__(self, current_loss: float, model: keras.Model, epoch: int = 0) -> bool:\n",
    "        \"\"\"\n",
    "        V√©rifie si l'entra√Ænement doit s'arr√™ter.\n",
    "        \n",
    "        Returns:\n",
    "            True si l'entra√Ænement doit s'arr√™ter, False sinon\n",
    "        \"\"\"\n",
    "        if self._is_improvement(current_loss):\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "            self.best_epoch = epoch\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.get_weights()\n",
    "            return False\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                if self.restore_best_weights and self.best_weights is not None:\n",
    "                    logger.info(f\"Restauration des poids de l'√©poque {self.best_epoch}\")\n",
    "                    model.set_weights(self.best_weights)\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def get_best_info(self) -> Dict:\n",
    "        \"\"\"Retourne les informations sur le meilleur mod√®le.\"\"\"\n",
    "        return {\n",
    "            'best_loss': self.best_loss,\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'stopped_epoch': self.stopped_epoch\n",
    "        }\n",
    "\n",
    "# =================================================================================\n",
    "# FONCTIONS DE PERTE\n",
    "# =================================================================================\n",
    "def entropic_risk_loss(lam: float = 0.1):\n",
    "    \"\"\"\n",
    "    Entropic risk measure avec stabilit√© num√©rique am√©lior√©e.\n",
    "    \n",
    "    Args:\n",
    "        lam: Param√®tre d'aversion au risque (plus √©lev√© = plus conservateur)\n",
    "    \n",
    "    Returns:\n",
    "        Fonction de perte TensorFlow\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def loss(y_true, PnL):\n",
    "        # Ne consid√©rer que les pertes (PnL > 0)\n",
    "        losses = tf.maximum(PnL, 0.0)\n",
    "        \n",
    "        # Stabilisation num√©rique\n",
    "        max_loss = tf.reduce_max(losses)\n",
    "        exp_term = tf.exp(lam * (losses - max_loss))\n",
    "        mean_exp = tf.reduce_mean(exp_term)\n",
    "        \n",
    "        # √âviter log(0)\n",
    "        log_term = tf.math.log(mean_exp + 1e-8)\n",
    "        \n",
    "        return (1.0 / (lam + 1e-8)) * (log_term + max_loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def cvar_loss(alpha: float = 0.99):\n",
    "    \"\"\"\n",
    "    Conditional Value at Risk (CVaR) loss.\n",
    "    \n",
    "    Args:\n",
    "        alpha: Niveau de quantile (0.90-0.999)\n",
    "    \n",
    "    Returns:\n",
    "        Fonction de perte TensorFlow\n",
    "    \"\"\"\n",
    "    @tf.function\n",
    "    def loss(y_true, PnL):\n",
    "        # Calculer le VaR (quantile)\n",
    "        sorted_pnl = tf.sort(PnL, direction='ASCENDING')\n",
    "        n = tf.shape(sorted_pnl)[0]\n",
    "        var_idx = tf.cast(tf.cast(n, tf.float32) * alpha, tf.int32)\n",
    "        var = sorted_pnl[var_idx]\n",
    "        \n",
    "        # CVaR = moyenne des pertes au-del√† du VaR\n",
    "        excess = tf.maximum(PnL - var, 0.0)\n",
    "        cvar = var + tf.reduce_mean(excess) / (1.0 - alpha + 1e-8)\n",
    "        \n",
    "        return cvar\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# =================================================================================\n",
    "# CLASSE PRINCIPALE\n",
    "# =================================================================================\n",
    "class DeepHedgingEngine:\n",
    "    \"\"\"\n",
    "    Moteur de deep hedging optimis√© pour produits GMAB.\n",
    "    Version am√©lior√©e pour la finance de march√©.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialisation avec validation des param√®tres.\"\"\"\n",
    "        try:\n",
    "            self._validate_config(config)\n",
    "            self.config = config\n",
    "            self.model = None\n",
    "            self.normalizer = DataNormalizer(method='standard')\n",
    "            self.early_stopping = None\n",
    "            self.training_history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'learning_rates': [],\n",
    "                'epoch_times': []\n",
    "            }\n",
    "            logger.info(f\"‚úì DeepHedgingEngine initialis√© - {config.model_type.value}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚úó Erreur d'initialisation: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _validate_config(self, config: ModelConfig) -> None:\n",
    "        \"\"\"Valide la configuration avec messages d√©taill√©s.\"\"\"\n",
    "        validations = [\n",
    "            (config.n_assets > 0, f\"n_assets doit √™tre > 0, re√ßu {config.n_assets}\"),\n",
    "            (config.n_features > 0, f\"n_features doit √™tre > 0, re√ßu {config.n_features}\"),\n",
    "            (config.hidden_layers > 0, f\"hidden_layers doit √™tre > 0, re√ßu {config.hidden_layers}\"),\n",
    "            (config.units > 0, f\"units doit √™tre > 0, re√ßu {config.units}\"),\n",
    "            (0 < config.learning_rate < 1, f\"learning_rate doit √™tre entre 0 et 1, re√ßu {config.learning_rate}\"),\n",
    "            (0 < config.alpha_cvar < 1, f\"alpha_cvar doit √™tre entre 0 et 1, re√ßu {config.alpha_cvar}\"),\n",
    "            (config.lambda_cost >= 0, f\"lambda_cost doit √™tre >= 0, re√ßu {config.lambda_cost}\"),\n",
    "            (0 <= config.dropout_rate < 1, f\"dropout_rate doit √™tre entre 0 et 1, re√ßu {config.dropout_rate}\"),\n",
    "        ]\n",
    "        \n",
    "        for condition, message in validations:\n",
    "            if not condition:\n",
    "                raise ValueError(message)\n",
    "    \n",
    "    def _build_model(self) -> keras.Model:\n",
    "        \"\"\"Construit le mod√®le de r√©seau neuronal.\"\"\"\n",
    "        logger.info(f\"üî® Construction du mod√®le {self.config.model_type.value}\")\n",
    "        \n",
    "        inputs = layers.Input(shape=(None, self.config.n_features), name='market_inputs')\n",
    "        x = inputs\n",
    "        \n",
    "        # Batch Normalization initiale (optionnel)\n",
    "        if self.config.batch_normalization:\n",
    "            x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "        \n",
    "        # Architecture selon le type\n",
    "        if self.config.model_type == ModelType.FFNN:\n",
    "            x = self._build_ffnn(x)\n",
    "        elif self.config.model_type == ModelType.LSTM:\n",
    "            x = self._build_lstm(x)\n",
    "        elif self.config.model_type == ModelType.GRU:\n",
    "            x = self._build_gru(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Type de mod√®le non support√©: {self.config.model_type}\")\n",
    "        \n",
    "        # Couche de sortie\n",
    "        outputs = layers.TimeDistributed(\n",
    "            layers.Dense(\n",
    "                self.config.n_assets,\n",
    "                activation='tanh',\n",
    "                kernel_initializer='glorot_uniform',\n",
    "                name='hedging_positions'\n",
    "            )\n",
    "        )(x)\n",
    "        \n",
    "        model = keras.Model(\n",
    "            inputs=inputs, \n",
    "            outputs=outputs, \n",
    "            name=f'DeepHedging_{self.config.model_type.value}'\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"‚úì Mod√®le construit - Param√®tres: {model.count_params():,}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _build_ffnn(self, x):\n",
    "        \"\"\"Construit architecture FFNN.\"\"\"\n",
    "        for i in range(self.config.hidden_layers):\n",
    "            units = self.config.units // (2**i) if i > 0 else self.config.units\n",
    "            \n",
    "            x = layers.TimeDistributed(\n",
    "                layers.Dense(\n",
    "                    units=units,\n",
    "                    activation='relu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=keras.regularizers.l2(self.config.l2_regularization)\n",
    "                )\n",
    "            )(x)\n",
    "            \n",
    "            if self.config.batch_normalization:\n",
    "                x = layers.TimeDistributed(layers.BatchNormalization())(x)\n",
    "            \n",
    "            if i == self.config.hidden_layers - 1 and self.config.dropout_rate > 0:\n",
    "                x = layers.Dropout(self.config.dropout_rate)(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _build_lstm(self, x):\n",
    "        \"\"\"Construit architecture LSTM.\"\"\"\n",
    "        for i in range(self.config.hidden_layers):\n",
    "            units = self.config.units // (2**i) if i > 0 else self.config.units\n",
    "            \n",
    "            x = layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=True,\n",
    "                dropout=self.config.dropout_rate if i == self.config.hidden_layers - 1 else 0.0,\n",
    "                recurrent_dropout=self.config.recurrent_dropout if i == self.config.hidden_layers - 1 else 0.0,\n",
    "                kernel_regularizer=keras.regularizers.l2(self.config.l2_regularization),\n",
    "                recurrent_regularizer=keras.regularizers.l2(self.config.l2_regularization)\n",
    "            )(x)\n",
    "            \n",
    "            if self.config.batch_normalization:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _build_gru(self, x):\n",
    "        \"\"\"Construit architecture GRU.\"\"\"\n",
    "        for i in range(self.config.hidden_layers):\n",
    "            units = self.config.units // (2**i) if i > 0 else self.config.units\n",
    "            \n",
    "            x = layers.GRU(\n",
    "                units,\n",
    "                return_sequences=True,\n",
    "                dropout=self.config.dropout_rate if i == self.config.hidden_layers - 1 else 0.0,\n",
    "                recurrent_dropout=self.config.recurrent_dropout if i == self.config.hidden_layers - 1 else 0.0,\n",
    "                kernel_regularizer=keras.regularizers.l2(self.config.l2_regularization),\n",
    "                recurrent_regularizer=keras.regularizers.l2(self.config.l2_regularization)\n",
    "            )(x)\n",
    "            \n",
    "            if self.config.batch_normalization:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compile_model(self, model: keras.Model, loss_fn: Optional[Callable] = None):\n",
    "        \"\"\"Compile le mod√®le avec configuration optimis√©e.\"\"\"\n",
    "        if loss_fn is None:\n",
    "            loss_fn = cvar_loss(alpha=self.config.alpha_cvar)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=self.config.learning_rate,\n",
    "            decay_steps=1000,\n",
    "            decay_rate=0.9\n",
    "        )\n",
    "        \n",
    "        # Optimiseur avec gradient clipping\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule,\n",
    "            clipnorm=self.config.gradient_clip_norm\n",
    "        )\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "        logger.info(\"‚úì Mod√®le compil√©\")\n",
    "    \n",
    "    @tf.function\n",
    "    def _simulate_one_step(self,\n",
    "                          V_t: tf.Tensor,\n",
    "                          delta_pred: tf.Tensor,\n",
    "                          delta_t: tf.Tensor,\n",
    "                          inputs_t: tf.Tensor,\n",
    "                          inputs_tp1: tf.Tensor,\n",
    "                          r_t: tf.Tensor,\n",
    "                          dt: float,\n",
    "                          n_step: tf.Tensor = None) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Simule UN SEUL pas de temps du portefeuille auto-financ√©.\n",
    "        Optimis√© pour la finance de march√©.\n",
    "        \"\"\"\n",
    "        lambda_cost = self.config.lambda_cost\n",
    "        scale_factor_zc = self.config.scale_factor_zc\n",
    "        maturities_months = tf.constant([12, 60, 180, 240, 300], dtype=tf.int32)\n",
    "        if n_step is None:\n",
    "            n_step = tf.constant(0, dtype=tf.int32)\n",
    "        B = tf.shape(V_t)[0]\n",
    "        \n",
    "        # Extraction des actifs (d√©j√† en log dans inputs)\n",
    "        log_assets_t = inputs_t[:, :6]\n",
    "        assets_real_t = tf.exp(log_assets_t)\n",
    "        swaps_real_t = inputs_t[:, 6:7]  # Swap rate asset\n",
    "        assets_real_t = tf.concat([assets_real_t, swaps_real_t], axis=1)\n",
    "\n",
    "        log_assets_tp1 = inputs_tp1[:, :6]\n",
    "        assets_real_tp1 = tf.exp(log_assets_tp1)\n",
    "        swaps_real_tp1 = inputs_tp1[:, 6:7]  # Swap rate asset\n",
    "        assets_real_tp1 = tf.concat([assets_real_tp1, swaps_real_tp1], axis=1)\n",
    "\n",
    "        # Scaling des positions\n",
    "        scale_mask = tf.concat([\n",
    "            tf.ones((1,), dtype=tf.float32),  # Action\n",
    "            tf.ones((5,), dtype=tf.float32) * scale_factor_zc,  # 5 ZC\n",
    "            tf.ones((1,), dtype=tf.float32) * 100  # Swap rate asset\n",
    "        ], axis=0)\n",
    "        \n",
    "        delta_scaled = delta_t * scale_mask\n",
    "        delta_scaled_prev = delta_pred * scale_mask\n",
    "\n",
    "        # Gestion du roulement (optionnel, ici simplifi√©)\n",
    "        cash_flow_roll = tf.zeros((B,), dtype=tf.float32)\n",
    "        for i in range(5):\n",
    "            is_expiry = tf.equal(n_step + 1, maturities_months[i])\n",
    "            if tf.reduce_any(is_expiry):\n",
    "                sell_price = 1.0\n",
    "                buy_price = assets_real_tp1[:, i + 1]\n",
    "                roll_cf = (sell_price - buy_price) * delta_scaled_prev[:, i + 1]\n",
    "                cash_flow_roll += tf.where(is_expiry, roll_cf, 0.0)\n",
    "\n",
    "        \n",
    "        # Co√ªts de transaction proportionnels\n",
    "        delta_change = tf.abs(delta_scaled - delta_scaled_prev)\n",
    "        transaction_costs = lambda_cost * tf.reduce_sum(delta_change * assets_real_t, axis=1)\n",
    "        \n",
    "        # Gains/pertes de march√©\n",
    "        exp_r_dt = tf.exp(r_t * dt)\n",
    "        exp_r_dt_ext = tf.expand_dims(exp_r_dt, axis=1)\n",
    "        \n",
    "        price_changes = assets_real_tp1 - exp_r_dt_ext * assets_real_t\n",
    "        market_pnl = tf.reduce_sum(delta_scaled * price_changes, axis=1)\n",
    "        \n",
    "        # Actualisation du cash\n",
    "        cash_growth = exp_r_dt * V_t\n",
    "        \n",
    "        # Mise √† jour totale\n",
    "        V_next = cash_growth + market_pnl - transaction_costs + cash_flow_roll\n",
    "        \n",
    "        return V_next\n",
    "    \n",
    "    def create_dataset(self,\n",
    "                      inputs: np.ndarray,\n",
    "                      H: np.ndarray,\n",
    "                      r: np.ndarray,\n",
    "                      batch_size: int = 512,\n",
    "                      shuffle: bool = True) -> tf.data.Dataset:\n",
    "        \"\"\"Cr√©e un dataset TensorFlow optimis√©.\"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((inputs, H, r))\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "        \n",
    "        dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def train(self,\n",
    "              train_input: np.ndarray,\n",
    "              train_input_norm: np.ndarray,\n",
    "              H_train: np.ndarray,\n",
    "              r_train: np.ndarray,\n",
    "              V0_train: float,\n",
    "              V0_valid: float,\n",
    "              dt: float,\n",
    "              val_input: Optional[np.ndarray] = None,\n",
    "              val_input_norm: Optional[np.ndarray] = None,\n",
    "              H_val: Optional[np.ndarray] = None,\n",
    "              r_val: Optional[np.ndarray] = None,\n",
    "              epochs: int = 50,\n",
    "              batch_size: int = 1024,\n",
    "              patience: int = 10,\n",
    "              loss_fn: Optional[Callable] = None,\n",
    "              verbose: int = 1) -> Dict[str, list]:\n",
    "        \"\"\"\n",
    "        Entra√Ænement am√©lior√© avec validation et early stopping.\n",
    "        \"\"\"\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"üöÄ D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Construction et compilation du mod√®le\n",
    "        if self.model is None:\n",
    "            self.model = self._build_model()\n",
    "            self.compile_model(self.model, loss_fn=loss_fn)\n",
    "        \n",
    "        # Configuration early stopping\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            patience=patience,\n",
    "            min_delta=0.001,\n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Cr√©ation des datasets\n",
    "        train_dataset_model = self.create_dataset(\n",
    "            train_input_norm, H_train, r_train, batch_size, shuffle=False\n",
    "        )\n",
    "        train_dataset_sim = self.create_dataset(\n",
    "            train_input, H_train, r_train, batch_size, shuffle=False\n",
    "        )\n",
    "        train_dataset = tf.data.Dataset.zip((train_dataset_model, train_dataset_sim))\n",
    "        \n",
    "        # Validation\n",
    "        validation_enabled = all([\n",
    "            val_input is not None,\n",
    "            val_input_norm is not None,\n",
    "            H_val is not None,\n",
    "            r_val is not None\n",
    "        ])\n",
    "        \n",
    "        if validation_enabled:\n",
    "            val_dataset_model = self.create_dataset(\n",
    "                val_input_norm, H_val, r_val, batch_size, shuffle=False\n",
    "            )\n",
    "            val_dataset_sim = self.create_dataset(\n",
    "                val_input, H_val, r_val, batch_size, shuffle=False\n",
    "            )\n",
    "            val_dataset = tf.data.Dataset.zip((val_dataset_model, val_dataset_sim))\n",
    "            logger.info(\"‚úì Validation activ√©e\")\n",
    "        else:\n",
    "            val_dataset = None\n",
    "            logger.warning(\"‚ö† Validation d√©sactiv√©e\")\n",
    "        \n",
    "        # Boucle d'entra√Ænement\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # === PHASE D'ENTRA√éNEMENT ===\n",
    "            train_losses = []\n",
    "            for batch_model, batch_sim in train_dataset:\n",
    "                batch_input_norm, batch_H, batch_r = batch_model\n",
    "                batch_input_raw, _, _ = batch_sim\n",
    "                \n",
    "                B = tf.shape(batch_input_raw)[0]\n",
    "                T = tf.shape(batch_input_raw)[1] - 1\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    V_t = tf.ones((B,), dtype=tf.float32) * V0_train\n",
    "                    delta_pred = tf.zeros((B, self.config.n_assets), dtype=tf.float32)\n",
    "                    for t in range(T):\n",
    "                        # Input avec valeur portefeuille normalis√©e\n",
    "                        current_market = batch_input_norm[:, t, :]\n",
    "                        current_input = tf.concat([\n",
    "                            current_market,\n",
    "                            tf.expand_dims(V_t / V0_train, axis=1)\n",
    "                        ], axis=1)\n",
    "                        \n",
    "                        # Pr√©dire positions\n",
    "                        delta_t = self.model(\n",
    "                            tf.expand_dims(current_input, axis=1),\n",
    "                            training=True\n",
    "                        )\n",
    "                        delta_t = tf.squeeze(delta_t, axis=1)\n",
    "                        \n",
    "                        # Simuler un pas\n",
    "                        V_t = self._simulate_one_step(\n",
    "                            V_t, delta_pred, delta_t,\n",
    "                            batch_input_raw[:, t, :],\n",
    "                            batch_input_raw[:, t + 1, :],\n",
    "                            batch_r[:, t], dt\n",
    "                        )\n",
    "                        delta_pred = delta_t\n",
    "                    # Calcul de la perte\n",
    "                    PnL = batch_H - V_t\n",
    "                    Pnl_clipped= tf.clip_by_value(PnL, -1e4, 1e4)\n",
    "                    total_loss = self.model.loss(None, tf.expand_dims(Pnl_clipped, axis=1))\n",
    "                \n",
    "                # Mise √† jour des poids\n",
    "                grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "                self.model.optimizer.apply_gradients(\n",
    "                    zip(grads, self.model.trainable_variables)\n",
    "                )\n",
    "                \n",
    "                train_losses.append(total_loss.numpy())\n",
    "            \n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            self.training_history['train_loss'].append(float(avg_train_loss))\n",
    "            \n",
    "            # === PHASE DE VALIDATION ===\n",
    "            avg_val_loss = None\n",
    "            if validation_enabled:\n",
    "                val_losses = []\n",
    "                for batch_model, batch_sim in val_dataset:\n",
    "                    batch_input_norm, batch_H, batch_r = batch_model\n",
    "                    batch_input_raw, _, _ = batch_sim\n",
    "                    \n",
    "                    B = tf.shape(batch_input_norm)[0]\n",
    "                    T = tf.shape(batch_input_norm)[1] - 1\n",
    "                    \n",
    "                    V_t = tf.ones((B,), dtype=tf.float32) * V0_valid\n",
    "                    delta_pred = tf.zeros((B, self.config.n_assets), dtype=tf.float32)\n",
    "                    \n",
    "                    for t in range(T):\n",
    "                        current_market = batch_input_norm[:, t, :]\n",
    "                        current_input = tf.concat([\n",
    "                            current_market,\n",
    "                            tf.expand_dims(V_t / V0_valid, axis=1)\n",
    "                        ], axis=1)\n",
    "                        \n",
    "                        delta_t = self.model(\n",
    "                            tf.expand_dims(current_input, axis=1),\n",
    "                            training=False\n",
    "                        )\n",
    "                        delta_t = tf.squeeze(delta_t, axis=1)\n",
    "                        \n",
    "                        V_t = self._simulate_one_step(\n",
    "                            V_t, delta_pred, delta_t,\n",
    "                            batch_input_raw[:, t, :],\n",
    "                            batch_input_raw[:, t + 1, :],\n",
    "                            batch_r[:, t], dt\n",
    "                        )\n",
    "                        delta_pred = delta_t\n",
    "                    \n",
    "                    PnL = batch_H - V_t\n",
    "                    Pnl_clipped= tf.clip_by_value(PnL, -1e4, 1e4)\n",
    "                    \n",
    "\n",
    "                    total_loss = self.model.loss(None, tf.expand_dims(Pnl_clipped, axis=1))\n",
    "                    val_losses.append(total_loss.numpy())\n",
    "                \n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                self.training_history['val_loss'].append(float(avg_val_loss))\n",
    "                \n",
    "                # Early stopping\n",
    "                if self.early_stopping(avg_val_loss, self.model, epoch):\n",
    "                    logger.info(f\"üõë Early stopping √† l'√©poque {epoch + 1}\")\n",
    "                    break\n",
    "            \n",
    "            # === LOGGING ===\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            self.training_history['epoch_times'].append(float(epoch_time))\n",
    "            \n",
    "            try:\n",
    "                lr = float(self.model.optimizer.learning_rate(\n",
    "                    self.model.optimizer.iterations\n",
    "                ))\n",
    "            except:\n",
    "                lr = float(self.model.optimizer.learning_rate)\n",
    "            \n",
    "            self.training_history['learning_rates'].append(lr)\n",
    "            \n",
    "            if verbose > 0 and ((epoch + 1) % max(1, epochs // 20) == 0 or epoch == 0):\n",
    "                log_msg = f\"üìä Epoch {epoch + 1:3d}/{epochs} ‚îÇ \"\n",
    "                log_msg += f\"Train: {avg_train_loss:.4f} ‚îÇ \"\n",
    "                if avg_val_loss is not None:\n",
    "                    log_msg += f\"Val: {avg_val_loss:.4f} ‚îÇ \"\n",
    "                log_msg += f\"LR: {lr:.2e} ‚îÇ \"\n",
    "                log_msg += f\"‚è± {epoch_time:.1f}s\"\n",
    "                logger.info(log_msg)\n",
    "        \n",
    "        # R√©sum√© final\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"‚úÖ ENTRA√éNEMENT TERMIN√â\")\n",
    "        if validation_enabled:\n",
    "            best_info = self.early_stopping.get_best_info()\n",
    "            logger.info(f\"   Meilleure val loss: {best_info['best_loss']:.4f}\")\n",
    "            logger.info(f\"   Meilleure √©poque: {best_info['best_epoch']}\")\n",
    "        total_time = sum(self.training_history['epoch_times'])\n",
    "        logger.info(f\"   Temps total: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        return self.training_history\n",
    "    \n",
    "    def predict(self,\n",
    "                test_input: np.ndarray,\n",
    "                test_input_norm: np.ndarray,\n",
    "                r_test: np.ndarray,\n",
    "                dt: float,\n",
    "                V0: float,\n",
    "                return_trajectory: bool = False) -> Tuple:\n",
    "        \"\"\"Pr√©diction avec le mod√®le entra√Æn√©.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Le mod√®le doit √™tre entra√Æn√© avant la pr√©diction\")\n",
    "        \n",
    "        logger.info(\"üîÆ G√©n√©ration des pr√©dictions...\")\n",
    "        \n",
    "        B = test_input.shape[0]\n",
    "        T = test_input.shape[1] - 1\n",
    "        \n",
    "        # Convertir en tenseurs\n",
    "        test_input_tf = tf.constant(test_input, dtype=tf.float32)\n",
    "        test_input_norm_tf = tf.constant(test_input_norm, dtype=tf.float32)\n",
    "        r_test_tf = tf.constant(r_test, dtype=tf.float32)\n",
    "        \n",
    "        V_t = tf.ones((B,), dtype=tf.float32) * V0\n",
    "        delta_prev = tf.zeros((B, self.config.n_assets), dtype=tf.float32)\n",
    "        \n",
    "        all_deltas = []\n",
    "        V_trajectory = [V_t.numpy()] if return_trajectory else None\n",
    "        print(T)\n",
    "        for t in range(T):\n",
    "            # Pr√©parer input\n",
    "            current_input = tf.concat([\n",
    "                test_input_norm_tf[:, t, :],\n",
    "                tf.expand_dims(V_t / V0, axis=1)\n",
    "            ], axis=1)\n",
    "            \n",
    "            # Pr√©dire positions\n",
    "            delta_t = self.model(\n",
    "                tf.expand_dims(current_input, axis=1),\n",
    "                training=False\n",
    "            )\n",
    "            delta_t = tf.squeeze(delta_t, axis=1)\n",
    "            all_deltas.append(delta_t)\n",
    "            \n",
    "            # Simuler un pas\n",
    "            V_t = self._simulate_one_step(\n",
    "                V_t, delta_prev, delta_t,\n",
    "                test_input_tf[:, t, :],\n",
    "                test_input_tf[:, t + 1, :],\n",
    "                r_test_tf[:, t], dt\n",
    "            )\n",
    "            \n",
    "            if return_trajectory:\n",
    "                V_trajectory.append(V_t.numpy())\n",
    "            \n",
    "            delta_prev = delta_t\n",
    "        \n",
    "        V_T = V_t.numpy()\n",
    "        delta_pred = tf.stack(all_deltas, axis=1).numpy()\n",
    "        \n",
    "        logger.info(\"‚úì Pr√©dictions g√©n√©r√©es\")\n",
    "        \n",
    "        if return_trajectory:\n",
    "            V_trajectory = np.array(V_trajectory).T\n",
    "            return V_T, delta_pred, V_trajectory\n",
    "        \n",
    "        return V_T, delta_pred\n",
    "    \n",
    "    def evaluate(self,\n",
    "                 test_input: np.ndarray,\n",
    "                 test_input_norm: np.ndarray,\n",
    "                 H_test: np.ndarray,\n",
    "                 r_test: np.ndarray,\n",
    "                 dt: float,\n",
    "                 V0: float,\n",
    "                 detailed: bool = True) -> Dict[str, float]:\n",
    "        \"\"\"√âvaluation compl√®te du mod√®le.\"\"\"\n",
    "        logger.info(\"üìä √âvaluation du mod√®le...\")\n",
    "        \n",
    "        V_T, delta_pred, V_trajectory = self.predict(\n",
    "            test_input, test_input_norm, r_test, dt, V0, return_trajectory=True\n",
    "        )\n",
    "        \n",
    "        PnL = H_test - V_T\n",
    "        # Filtrer les outliers extr√™mes pour l'affichage\n",
    "        #mask = np.abs(PnL) < 1e4\n",
    "        PnL_filtered = PnL #[mask]\n",
    "        # M√©triques de base\n",
    "        results = {\n",
    "            'mean_pnl': float(np.mean(PnL_filtered)),\n",
    "            'std_pnl': float(np.std(PnL_filtered)),\n",
    "            'min_pnl': float(np.min(PnL_filtered)),\n",
    "            'max_pnl': float(np.max(PnL_filtered)),\n",
    "            'median_pnl': float(np.median(PnL_filtered)),\n",
    "        }\n",
    "        \n",
    "        # M√©triques de risque\n",
    "        for alpha in [0.90, 0.95, 0.99]:\n",
    "            var = np.percentile(PnL_filtered, alpha * 100)\n",
    "            cvar = np.mean(PnL_filtered[PnL_filtered >= var])\n",
    "            results[f'var_{int(alpha * 100)}'] = float(var)\n",
    "            results[f'cvar_{int(alpha * 100)}'] = float(cvar)\n",
    "        \n",
    "        # M√©triques avanc√©es\n",
    "        if detailed:\n",
    "            results['skewness'] = float(stats.skew(PnL_filtered))\n",
    "            results['kurtosis'] = float(stats.kurtosis(PnL_filtered))\n",
    "            results['sharpe_ratio'] = float(np.mean(PnL_filtered) / (np.std(PnL_filtered) + 1e-8))\n",
    "            \n",
    "            # Efficacit√© de la couverture\n",
    "            hedge_efficiency = 1 - (np.std(PnL_filtered) / (np.std(H_test) + 1e-8))\n",
    "            results['efficiency'] = float(hedge_efficiency)\n",
    "            \n",
    "            # Taux de succ√®s (PnL < 0)\n",
    "            success_rate = np.mean(PnL_filtered < 0)\n",
    "            results['success_rate'] = float(success_rate)\n",
    "        \n",
    "        # Affichage des r√©sultats\n",
    "        logger.info(\"\\n\" + \"=\" * 80)\n",
    "        logger.info(\"üìà R√âSULTATS D'√âVALUATION\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "        for key, value in results.items():\n",
    "            logger.info(f\"  {key:.<30} {value:>15.4f}\")\n",
    "        \n",
    "        logger.info(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        return results,V_T, delta_pred, V_trajectory,PnL_filtered\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Sauvegarde compl√®te du mod√®le.\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"üíæ Sauvegarde du mod√®le dans {path}\")\n",
    "        \n",
    "        # Sauvegarder les poids\n",
    "        model_path = os.path.join(path, \"model.weights.h5\")\n",
    "        self.model.save_weights(model_path)\n",
    "        \n",
    "        # Sauvegarder la configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config.to_dict(), f, indent=2)\n",
    "        \n",
    "        # Sauvegarder l'historique\n",
    "        history_path = os.path.join(path, \"training_history.json\")\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.training_history, f, indent=2)\n",
    "        \n",
    "        # Sauvegarder le normaliseur\n",
    "        if self.normalizer.is_fitted:\n",
    "            normalizer_path = os.path.join(path, \"normalizer.npz\")\n",
    "            self.normalizer.save(normalizer_path)\n",
    "        \n",
    "        logger.info(f\"‚úì Mod√®le sauvegard√© avec succ√®s\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'DeepHedgingEngine':\n",
    "        \"\"\"Charge un mod√®le sauvegard√©.\"\"\"\n",
    "        logger.info(f\"üìÇ Chargement du mod√®le depuis {path}\")\n",
    "        \n",
    "        # Charger la configuration\n",
    "        config_path = os.path.join(path, \"config.json\")\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        config = ModelConfig.from_dict(config_dict)\n",
    "        \n",
    "        # Cr√©er l'instance\n",
    "        engine = cls(config)\n",
    "        engine.model = engine._build_model()\n",
    "        \n",
    "        # Charger les poids\n",
    "        model_path = os.path.join(path, \"model.weights.h5\")\n",
    "        engine.model.load_weights(model_path)\n",
    "        \n",
    "        # Charger l'historique\n",
    "        history_path = os.path.join(path, \"training_history.json\")\n",
    "        if os.path.exists(history_path):\n",
    "            with open(history_path, 'r') as f:\n",
    "                engine.training_history = json.load(f)\n",
    "        \n",
    "        # Charger le normaliseur\n",
    "        normalizer_path = os.path.join(path, \"normalizer.npz\")\n",
    "        if os.path.exists(normalizer_path):\n",
    "            engine.normalizer = DataNormalizer.load(normalizer_path)\n",
    "        \n",
    "        logger.info(f\"‚úì Mod√®le charg√© avec succ√®s\")\n",
    "        return engine\n",
    "\n",
    "# =================================================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# =================================================================================\n",
    "def compute_metrics(PnL: np.ndarray, alpha: float = 0.99) -> Tuple[float, float, float]:\n",
    "    \"\"\"Calcule les m√©triques de risque.\"\"\"\n",
    "    mean = np.mean(PnL)\n",
    "    var = np.percentile(PnL, alpha * 100)\n",
    "    cvar = np.mean(PnL[PnL >= var])\n",
    "    return mean, var, cvar\n",
    "\n",
    "\n",
    "def roll_zc_paths(P_paths: np.ndarray,\n",
    "                  maturities: list,\n",
    "                  N_per_year: int = 12) -> np.ndarray:\n",
    "    \"\"\"Roule les z√©ro-coupons expir√©s.\"\"\"\n",
    "    M, T_plus1, n_zc = P_paths.shape\n",
    "    P_paths_rolled = P_paths.copy()\n",
    "    \n",
    "    for i, maturity in enumerate(maturities):\n",
    "        expiry_step = int(maturity * N_per_year)\n",
    "        \n",
    "        for m in range(M):\n",
    "            t = expiry_step\n",
    "            while t < T_plus1:\n",
    "                next_expiry = t + expiry_step\n",
    "                if next_expiry >= T_plus1:\n",
    "                    break\n",
    "                \n",
    "                steps_to_copy = min(expiry_step, T_plus1 - t)\n",
    "                P_paths_rolled[m, t:t + steps_to_copy, i] = P_paths[m, :steps_to_copy, i]\n",
    "                t = next_expiry\n",
    "    \n",
    "    return P_paths_rolled\n",
    "\n",
    "\n",
    "def compute_realized_volatility(S_paths: np.ndarray, window: int = 20) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcule la volatilit√© r√©alis√©e sur une fen√™tre glissante.\n",
    "    \n",
    "    Args:\n",
    "        S_paths: (M, T+1) trajectoires de l'indice\n",
    "        window: nombre de pas pour la fen√™tre\n",
    "    \n",
    "    Returns:\n",
    "        sigma_realized: (M, T+1) volatilit√© r√©alis√©e\n",
    "    \"\"\"\n",
    "    M, T_plus1 = S_paths.shape\n",
    "    log_returns = np.log(S_paths[:, 1:] / S_paths[:, :-1])\n",
    "    \n",
    "    sigma_realized = np.zeros((M, T_plus1), dtype=np.float32)\n",
    "    \n",
    "    for t in range(T_plus1):\n",
    "        if t == 0:\n",
    "            sigma_realized[:, t] = np.std(log_returns[:, :1], axis=1) + 1e-6\n",
    "        else:\n",
    "            start = max(0, t - window)\n",
    "            end = t\n",
    "            if end > start:\n",
    "                sigma_realized[:, t] = np.std(log_returns[:, start:end], axis=1) + 1e-6\n",
    "            else:\n",
    "                sigma_realized[:, t] = sigma_realized[:, t - 1]\n",
    "    \n",
    "    return sigma_realized\n",
    "\n",
    "\n",
    "def plot_training_history(history: Dict[str, list],\n",
    "                          save_path: Optional[str] = None,\n",
    "                          show: bool = True) -> None:\n",
    "    \"\"\"Visualise l'historique d'entra√Ænement.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette(\"colorblind\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Historique d\\'entra√Ænement', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs = np.arange(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # 1. Loss curves\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, history['train_loss'], label='Train', linewidth=2)\n",
    "    if 'val_loss' in history and len(history['val_loss']) > 0:\n",
    "        ax.plot(epochs, history['val_loss'], label='Validation', linewidth=2)\n",
    "    ax.set_xlabel('√âpoque')\n",
    "    ax.set_ylabel('Perte')\n",
    "    ax.set_title('Fonction de perte')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Learning rate\n",
    "    ax = axes[0, 1]\n",
    "    if 'learning_rates' in history and len(history['learning_rates']) > 0:\n",
    "        ax.plot(epochs, history['learning_rates'], color='green', linewidth=2)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('√âpoque')\n",
    "        ax.set_ylabel('Learning Rate (log)')\n",
    "        ax.set_title('Taux d\\'apprentissage')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Temps par √©poque\n",
    "    ax = axes[1, 0]\n",
    "    if 'epoch_times' in history and len(history['epoch_times']) > 0:\n",
    "        ax.plot(epochs, history['epoch_times'], color='purple', linewidth=2)\n",
    "        ax.axhline(np.mean(history['epoch_times']), color='red', linestyle='--',\n",
    "                  label=f'Moyenne: {np.mean(history[\"epoch_times\"]):.2f}s')\n",
    "        ax.set_xlabel('√âpoque')\n",
    "        ax.set_ylabel('Temps (s)')\n",
    "        ax.set_title('Temps par √©poque')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Diff√©rence Train/Val\n",
    "    ax = axes[1, 1]\n",
    "    if ('val_loss' in history and len(history['val_loss']) > 0 and\n",
    "        len(history['train_loss']) == len(history['val_loss'])):\n",
    "        diff = np.array(history['train_loss']) - np.array(history['val_loss'])\n",
    "        ax.plot(epochs, diff, color='orange', linewidth=2)\n",
    "        ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.fill_between(epochs, diff, 0, where=(diff > 0), alpha=0.3, color='red')\n",
    "        ax.fill_between(epochs, diff, 0, where=(diff <= 0), alpha=0.3, color='green')\n",
    "        ax.set_xlabel('√âpoque')\n",
    "        ax.set_ylabel('Train - Val')\n",
    "        ax.set_title('Surapprentissage')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"‚úì Graphique sauvegard√©: {save_path}\")\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_detailed_results(PnL_test: np.ndarray,\n",
    "                          delta_test: np.ndarray,\n",
    "                          L_t: np.ndarray,\n",
    "                          V_test: np.ndarray,\n",
    "                          T_years: float = 25,\n",
    "                          save_path: Optional[str] = None,\n",
    "                          show: bool = True) -> None:\n",
    "    \"\"\"Visualisations d√©taill√©es.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    fig.suptitle('Analyse d√©taill√©e de la couverture', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    t_full = np.linspace(0, T_years, L_t.shape[1] if L_t.ndim == 2 else len(L_t))\n",
    "    t_hedge = np.linspace(0, T_years, delta_test.shape[1])\n",
    "    \n",
    "    # 1. Distribution P&L\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(PnL_test, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "    sns.kdeplot(PnL_test, ax=ax, color='darkblue', linewidth=2)\n",
    "    ax.axvline(np.mean(PnL_test), color='red', linestyle='--', linewidth=2,\n",
    "              label=f'Moyenne: {np.mean(PnL_test):.2f}')\n",
    "    ax.axvline(0, color='green', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('P&L')\n",
    "    ax.set_ylabel('Densit√©')\n",
    "    ax.set_title('Distribution du P&L')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. QQ-Plot\n",
    "    stats.probplot(PnL_test, dist=\"norm\", plot=axes[0, 1])\n",
    "    axes[0, 1].set_title('QQ-Plot')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Passif\n",
    "    ax = axes[0, 2]\n",
    "    if L_t.ndim == 2:\n",
    "        L_mean = np.mean(L_t, axis=0)\n",
    "        L_5 = np.percentile(L_t, 5, axis=0)\n",
    "        L_95 = np.percentile(L_t, 95, axis=0)\n",
    "        ax.plot(t_full, L_mean, color='darkred', linewidth=2, label='Moyenne')\n",
    "        ax.fill_between(t_full, L_5, L_95, color='red', alpha=0.2)\n",
    "    else:\n",
    "        ax.plot(t_full, L_t, color='darkred', linewidth=2)\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Valeur')\n",
    "    ax.set_title('√âvolution du passif')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Positions moyennes\n",
    "    ax = axes[1, 0]\n",
    "    asset_names = ['Actions', 'ZC 1Y', 'ZC 5Y', 'ZC 15Y', 'ZC 20Y', 'ZC 25Y'] #, 'Swap'\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(asset_names)))\n",
    "    \n",
    "    for i in range(min(delta_test.shape[2], len(asset_names))):\n",
    "        mean_pos = np.mean(delta_test[:, :, i], axis=0)\n",
    "        ax.plot(t_hedge, mean_pos, label=asset_names[i], color=colors[i], linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_title('Strat√©gie de couverture')\n",
    "    ax.legend(fontsize=8, ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Actif vs Passif\n",
    "    ax = axes[1, 1]\n",
    "    if L_t.ndim == 2 and V_test.ndim == 2:\n",
    "        L_plot = np.mean(L_t, axis=0)\n",
    "        V_mean = np.mean(V_test, axis=0)\n",
    "        ax.plot(t_full, L_plot, color='darkred', linewidth=2, label='Passif', linestyle='--')\n",
    "        ax.plot(t_full, V_mean, color='darkblue', linewidth=2, label='Actif')\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Valeur')\n",
    "    ax.set_title('Actif vs Passif')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. √âcart\n",
    "    ax = axes[1, 2]\n",
    "    if L_t.ndim == 2 and V_test.ndim == 2:\n",
    "        diff = V_test - L_t\n",
    "        diff_mean = np.mean(diff, axis=0)\n",
    "        ax.plot(t_full, diff_mean, color='purple', linewidth=2)\n",
    "        ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('√âcart')\n",
    "    ax.set_title('√âcart Actif - Passif')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Corr√©lations\n",
    "    ax = axes[2, 0]\n",
    "    if delta_test.ndim == 3:\n",
    "        delta_mean = np.mean(delta_test, axis=0)\n",
    "        corr = np.corrcoef(delta_mean.T)\n",
    "        im = ax.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "        ax.set_xticks(range(len(asset_names)))\n",
    "        ax.set_yticks(range(len(asset_names)))\n",
    "        ax.set_xticklabels(asset_names, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_yticklabels(asset_names, fontsize=8)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        ax.set_title('Corr√©lations')\n",
    "    \n",
    "    # 8. Volatilit√©\n",
    "    ax = axes[2, 1]\n",
    "    for i in range(min(delta_test.shape[2], len(asset_names))):\n",
    "        std_pos = np.std(delta_test[:, :, i], axis=0)\n",
    "        ax.plot(t_hedge, std_pos, color=colors[i], linewidth=2, label=asset_names[i])\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('√âcart-type')\n",
    "    ax.set_title('Volatilit√© des positions')\n",
    "    ax.legend(fontsize=8, ncol=2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Distribution finale\n",
    "    ax = axes[2, 2]\n",
    "    if L_t.ndim == 2:\n",
    "        L_final = L_t[:, -1]\n",
    "        V_final = V_test[:, -1] if V_test.ndim == 2 else V_test\n",
    "        ax.hist(L_final, bins=30, alpha=0.6, label='Passif', density=True)\n",
    "        ax.hist(V_final, bins=30, alpha=0.6, label='Actif', density=True)\n",
    "        ax.set_xlabel('Valeur')\n",
    "        ax.set_ylabel('Densit√©')\n",
    "        ax.set_title('Distribution finale')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"‚úì Graphique sauvegard√©: {save_path}\")\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def print_metrics_table(results: Dict[str, float]) -> None:\n",
    "    \"\"\"Affiche une table format√©e des m√©triques.\"\"\"\n",
    "    try:\n",
    "        from tabulate import tabulate\n",
    "        \n",
    "        headers = [\"M√©trique\", \"Valeur\"]\n",
    "        rows = []\n",
    "        \n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, float):\n",
    "                formatted_value = f\"{value:.4f}\"\n",
    "                rows.append([key.replace('_', ' ').title(), formatted_value])\n",
    "        \n",
    "        table = tabulate(rows, headers=headers, tablefmt=\"fancy_grid\")\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä R√âSULTATS DE PERFORMANCE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(table)\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä R√âSULTATS DE PERFORMANCE\")\n",
    "        print(\"=\" * 80)\n",
    "        for key, value in results.items():\n",
    "            print(f\"  {key:.<40} {value:>15.4f}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f90330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================\n",
    "#Mod√©lisation de la survie du contrat d'assurance vie\n",
    "#==================================================================================\n",
    "\"\"\"\n",
    "Mod√©lisation de la survie et des rachats pour contrats d'assurance vie\n",
    "Mod√®le : Makeham + Rachats d√©pendants de la moneyness\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =================================================================================\n",
    "# CONFIGURATION\n",
    "# =================================================================================\n",
    "np.random.seed(42)\n",
    "\n",
    "# =================================================================================\n",
    "# TABLES DE MORTALIT√â\n",
    "# =================================================================================\n",
    "def create_mortality_table(table_type='TD88-90'):\n",
    "    \"\"\"\n",
    "    Cr√©e une table de mortalit√©.\n",
    "    \n",
    "    Args:\n",
    "        table_type: 'TD88-90' (approximation) ou 'custom'\n",
    "    \n",
    "    Returns:\n",
    "        qx_annual: Probabilit√©s de d√©c√®s annuelles par √¢ge (0-120)\n",
    "    \"\"\"\n",
    "    ages = np.arange(0, 121)\n",
    "    \n",
    "    if table_type == 'TD88-90':\n",
    "        # Approximation de TD88-90 (table fran√ßaise vie)\n",
    "        # Formule de Makeham simplifi√©e\n",
    "        qx_annual = 0.0005 + 0.00005 * (1.09 ** ages)\n",
    "        qx_annual = np.clip(qx_annual, 0, 1.0)\n",
    "    \n",
    "    elif table_type == 'Gompertz':\n",
    "        # Loi de Gompertz pure\n",
    "        alpha, beta = 0.0001, 0.10\n",
    "        qx_annual = 1 - np.exp(-alpha * np.exp(beta * ages))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Type de table inconnu: {table_type}\")\n",
    "    \n",
    "    return qx_annual\n",
    "\n",
    "\n",
    "def annual_to_monthly_qx(qx_annual):\n",
    "    \"\"\"\n",
    "    Convertit les probabilit√©s annuelles en mensuelles.\n",
    "    \n",
    "    Formule : q_x^(12) = 1 - (1 - q_x)^(1/12)\n",
    "    \n",
    "    Hypoth√®se : Force de mortalit√© constante sur l'ann√©e.\n",
    "    \"\"\"\n",
    "    return 1 - (1 - qx_annual) ** (1/12)\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# MOD√àLE DE MAKEHAM\n",
    "# =================================================================================\n",
    "def makeham_survival(t, x, a, b, c):\n",
    "    \"\"\"\n",
    "    Probabilit√© de survie {}_t p_x avec la loi de Makeham.\n",
    "    \n",
    "    Force de mortalit√© : Œº(x) = a + b * c^x\n",
    "    \n",
    "    Survie : p_t = exp(-‚à´[0,t] Œº(x+s) ds)\n",
    "    \n",
    "    Args:\n",
    "        t: Dur√©e (ann√©es)\n",
    "        x: √Çge initial\n",
    "        a, b, c: Param√®tres de Makeham\n",
    "    \n",
    "    Returns:\n",
    "        Probabilit√© de survie de x √† x+t\n",
    "    \"\"\"\n",
    "    # Terme 1 : exp(-a*t)\n",
    "    term1 = np.exp(-a * t)\n",
    "    \n",
    "    # Terme 2 : exp(-(b/log(c)) * (c^(x+t) - c^x))\n",
    "    if c == 1:\n",
    "        # Cas d√©g√©n√©r√© (Gompertz avec c=1)\n",
    "        term2 = np.exp(-b * t)\n",
    "    else:\n",
    "        log_c = np.log(c)\n",
    "        term2 = np.exp(-(b / log_c) * (c**(x + t) - c**x))\n",
    "    \n",
    "    return term1 * term2\n",
    "\n",
    "\n",
    "def makeham_force(x, a, b, c):\n",
    "    \"\"\"Force de mortalit√© Œº(x) = a + b*c^x.\"\"\"\n",
    "    return a + b * (c ** x)\n",
    "\n",
    "\n",
    "def calibrate_makeham(qx_monthly, x_start=40, x_end=100):\n",
    "    \"\"\"\n",
    "    Calibre les param√®tres de Makeham sur une table de mortalit√©.\n",
    "    \n",
    "    M√©thode : Minimisation de l'erreur quadratique entre q_x observ√© et pr√©dit.\n",
    "    \n",
    "    Args:\n",
    "        qx_monthly: Table de mortalit√© mensuelle\n",
    "        x_start, x_end: Plage d'√¢ges pour la calibration\n",
    "    \n",
    "    Returns:\n",
    "        (a, b, c): Param√®tres calibr√©s\n",
    "    \"\"\"\n",
    "    target_ages = np.arange(x_start, min(x_end, len(qx_monthly)))\n",
    "    target_qx = qx_monthly[target_ages]\n",
    "    \n",
    "    def loss(params):\n",
    "        a, b, c = params\n",
    "        \n",
    "        # Contraintes strictes\n",
    "        if a < 0 or b < 0 or c <= 1:\n",
    "            return 1e10\n",
    "        \n",
    "        # Force de mortalit√©\n",
    "        mu_x = a + b * (c ** target_ages)\n",
    "        \n",
    "        # Conversion en q_x mensuel : q ‚âà 1 - exp(-Œº/12)\n",
    "        pred_qx = 1 - np.exp(-mu_x / 12)\n",
    "        \n",
    "        # MSE\n",
    "        mse = np.mean((pred_qx - target_qx) ** 2)\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    # Optimisation\n",
    "    result = minimize(\n",
    "        loss, \n",
    "        x0=[0.0005, 0.00005, 1.09],\n",
    "        method='L-BFGS-B',\n",
    "        bounds=[(1e-6, 0.1), (1e-8, 0.1), (1.01, 1.2)]\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        print(f\"‚úì Calibration r√©ussie (MSE: {result.fun:.2e})\")\n",
    "    else:\n",
    "        print(f\"‚ö† Calibration partielle (MSE: {result.fun:.2e})\")\n",
    "    \n",
    "    return result.x\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# MOD√àLE DE RACHATS\n",
    "# =================================================================================\n",
    "def surrender_rate(rho, model='piecewise'):\n",
    "    \"\"\"\n",
    "    Taux de rachat œÄ_t en fonction de la moneyness œÅ_t = S_t / max(S).\n",
    "    \n",
    "    Mod√®le empirique observ√© sur les contrats d'assurance vie :\n",
    "    - Si œÅ < 0.7 (out-of-the-money) : peu de rachats (0.3%)\n",
    "    - Si 0.7 ‚â§ œÅ ‚â§ 1.4 : rachats croissants lin√©airement\n",
    "    - Si œÅ > 1.4 (in-the-money) : rachats √©lev√©s (2%)\n",
    "    \n",
    "    Args:\n",
    "        rho: Moneyness (ratio courant/max)\n",
    "        model: 'piecewise' ou 'smooth'\n",
    "    \n",
    "    Returns:\n",
    "        Taux de rachat mensuel\n",
    "    \"\"\"\n",
    "    if model == 'piecewise':\n",
    "        # Mod√®le par morceaux (discontinu)\n",
    "        if rho < 0.7:\n",
    "            return 0.003  # 0.3% par mois\n",
    "        elif rho <= 1.4:\n",
    "            return 0.003 + 0.0243 * (rho - 0.7)  # Lin√©aire\n",
    "        else:\n",
    "            return 0.020  # 2% par mois\n",
    "    \n",
    "    elif model == 'smooth':\n",
    "        # Mod√®le lisse (sigmo√Øde)\n",
    "        return 0.003 + 0.017 / (1 + np.exp(-10 * (rho - 1.0)))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Mod√®le inconnu: {model}\")\n",
    "\n",
    "\n",
    "def simulate_contract_reduction(S_path, x0, qx_monthly, a, b, c, surrender_model='piecewise'):\n",
    "    \"\"\"\n",
    "    Simule CR_t : fraction du portefeuille restante compte tenu des d√©c√®s et rachats.\n",
    "    \n",
    "    CR_t+1 = CR_t * (1 - q_x - œÄ_t + q_x * œÄ_t)\n",
    "    \n",
    "    Le terme (1 - q_x - œÄ_t + q_x * œÄ_t) repr√©sente la probabilit√© de rester\n",
    "    dans le portefeuille sachant qu'on peut :\n",
    "    - D√©c√©der (q_x)\n",
    "    - Racheter (œÄ_t)\n",
    "    - Les deux sont mutuellement exclusifs\n",
    "    \n",
    "    Args:\n",
    "        S_path: Trajectoire de l'indice\n",
    "        x0: √Çge initial\n",
    "        qx_monthly: Table de mortalit√© mensuelle\n",
    "        a, b, c: Param√®tres de Makeham\n",
    "        surrender_model: Type de mod√®le de rachats\n",
    "    \n",
    "    Returns:\n",
    "        CR_t: Facteur de r√©duction du portefeuille\n",
    "    \"\"\"\n",
    "    T = len(S_path)\n",
    "    CR = np.ones(T)\n",
    "    \n",
    "    # Calcul de la moneyness\n",
    "    running_max = np.maximum.accumulate(S_path)\n",
    "    rho = S_path / running_max\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # √Çge courant\n",
    "        age_months = t\n",
    "        age = x0 + age_months // 12\n",
    "        age = min(age, len(qx_monthly) - 1)  # Borne sup√©rieure\n",
    "        \n",
    "        # Probabilit√© de d√©c√®s mensuelle\n",
    "        qx = qx_monthly[age]\n",
    "        \n",
    "        # Taux de rachat\n",
    "        pi_t = surrender_rate(rho[t], model=surrender_model)\n",
    "        \n",
    "        # Mise √† jour de CR_t\n",
    "        # P(rester) = P(ne pas d√©c√©der ET ne pas racheter)\n",
    "        #           = (1 - q_x) * (1 - œÄ_t)\n",
    "        #           = 1 - q_x - œÄ_t + q_x * œÄ_t\n",
    "        CR[t] = CR[t-1] * (1 - qx - pi_t + qx * pi_t)\n",
    "    \n",
    "    return CR\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# VISUALISATION\n",
    "# =================================================================================\n",
    "def plot_survival_and_cr(t_vals, survival, CR_path, S_path):\n",
    "    \"\"\"Graphiques combin√©s : survie, CR_t et S_t.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Analyse de la survie et des rachats', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Courbe de survie Makeham\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(t_vals, survival, 'b-', linewidth=2)\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Probabilit√© de survie')\n",
    "    ax.set_title('Survie Makeham ${}_tp_x$')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Facteur de r√©duction CR_t\n",
    "    ax = axes[0, 1]\n",
    "    t_months = np.arange(len(CR_path)) / 12\n",
    "    ax.plot(t_months, CR_path, 'r-', linewidth=2)\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Fraction du portefeuille')\n",
    "    ax.set_title('Facteur de r√©duction $CR_t$')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Trajectoire de l'indice\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(t_months, S_path, 'g-', linewidth=1.5)\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Valeur de l\\'indice')\n",
    "    ax.set_title('Trajectoire de l\\'indice $S_t$')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Moneyness\n",
    "    ax = axes[1, 1]\n",
    "    running_max = np.maximum.accumulate(S_path)\n",
    "    rho = S_path / running_max\n",
    "    ax.plot(t_months, rho, 'm-', linewidth=1.5)\n",
    "    ax.axhline(0.7, color='orange', linestyle='--', alpha=0.5, label='Seuil bas')\n",
    "    ax.axhline(1.4, color='red', linestyle='--', alpha=0.5, label='Seuil haut')\n",
    "    ax.set_xlabel('Temps (ann√©es)')\n",
    "    ax.set_ylabel('Moneyness $\\\\rho_t$')\n",
    "    ax.set_title('Ratio $S_t / \\\\max_{s \\\\leq t} S_s$')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# =================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"SIMULATION DE LA SURVIE ET DES RACHATS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Cr√©ation de la table de mortalit√©\n",
    "    print(\"\\n1. Cr√©ation de la table de mortalit√©...\")\n",
    "    qx_annual = create_mortality_table(table_type='TD88-90')\n",
    "    qx_monthly = annual_to_monthly_qx(qx_annual)\n",
    "    print(f\"   Table cr√©√©e : {len(qx_annual)} √¢ges\")\n",
    "    \n",
    "    # 2. Calibration de Makeham\n",
    "    print(\"\\n2. Calibration du mod√®le de Makeham...\")\n",
    "    a, b, c = calibrate_makeham(qx_monthly, x_start=40, x_end=100)\n",
    "    print(f\"   Param√®tres : a={a:.6f}, b={b:.8f}, c={c:.6f}\")\n",
    "    \n",
    "    # 3. Courbe de survie th√©orique\n",
    "    print(\"\\n3. Calcul de la courbe de survie...\")\n",
    "    x0 = 40  # √Çge initial\n",
    "    T_years = 30\n",
    "    t_vals = np.linspace(0, T_years, T_years * 12 + 1)\n",
    "    survival = makeham_survival(t_vals, x0, a, b, c)\n",
    "    print(f\"   Survie √† 30 ans : {survival[-1]:.4f}\")\n",
    "    \n",
    "    # 4. Simulation d'une trajectoire d'actif\n",
    "    print(\"\\n4. Simulation d'une trajectoire GBM...\")\n",
    "    S0 = 100\n",
    "    mu = 0.05    # 5% de drift\n",
    "    sigma = 0.20  # 20% de volatilit√©\n",
    "    T_months = T_years * 12\n",
    "    S_path = simulate_GBM(S0, mu, sigma, T_months, dt=1/12)\n",
    "    print(f\"   S_0 = {S0:.2f}, S_T = {S_path[-1]:.2f}\")\n",
    "    \n",
    "    # 5. Calcul de CR_t\n",
    "    print(\"\\n5. Calcul du facteur de r√©duction CR_t...\")\n",
    "    CR_path = simulate_contract_reduction(\n",
    "        S_path, x0, qx_monthly, a, b, c, surrender_model='piecewise'\n",
    "    )\n",
    "    print(f\"   CR_T = {CR_path[-1]:.4f} ({100*(1-CR_path[-1]):.1f}% de sorties)\")\n",
    "    \n",
    "    # 6. D√©composition des sorties\n",
    "    print(\"\\n6. Analyse des sorties...\")\n",
    "    # Estimation approximative\n",
    "    theoretical_survival = makeham_survival(T_years, x0, a, b, c)\n",
    "    total_exit = 1 - CR_path[-1]\n",
    "    death_exit = 1 - theoretical_survival\n",
    "    surrender_exit = total_exit - death_exit\n",
    "    \n",
    "    print(f\"   Sorties totales    : {100*total_exit:.2f}%\")\n",
    "    print(f\"   ‚îî‚îÄ Par d√©c√®s       : {100*death_exit:.2f}%\")\n",
    "    print(f\"   ‚îî‚îÄ Par rachats     : {100*surrender_exit:.2f}%\")\n",
    "    \n",
    "    # 7. Visualisation\n",
    "    print(\"\\n7. G√©n√©ration des graphiques...\")\n",
    "    plot_survival_and_cr(t_vals, survival, CR_path, S_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Simulation termin√©e\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48590f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 14:23:44,990 - __main__ - INFO - üöÄ Deep Hedging GMAB - D√©marrage\n",
      "2026-02-03 14:23:44,992 - __main__ - INFO - üìÇ Chargement des donn√©es...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 14:24:14,571 - __main__ - INFO - ‚úì Donn√©es charg√©es: 50000 sc√©narios\n",
      "2026-02-03 14:28:13,094 - __main__ - INFO - üéØ Pr√©paration des features...\n",
      "2026-02-03 14:28:21,398 - __main__ - INFO - ‚úÇÔ∏è Division des donn√©es...\n",
      "2026-02-03 14:28:21,538 - __main__ - INFO - üìè Normalisation...\n",
      "2026-02-03 14:28:21,978 - __main__ - INFO - Normalizer fitted - method: standard\n",
      "2026-02-03 14:28:22,372 - __main__ - INFO - ‚öôÔ∏è Configuration du mod√®le...\n",
      "2026-02-03 14:28:22,377 - __main__ - INFO - ‚úì DeepHedgingEngine initialis√© - LSTM\n",
      "2026-02-03 14:28:22,506 - __main__ - INFO - ================================================================================\n",
      "2026-02-03 14:28:22,506 - __main__ - INFO - üöÄ D√âBUT DE L'ENTRA√éNEMENT\n",
      "2026-02-03 14:28:22,520 - __main__ - INFO - ================================================================================\n",
      "2026-02-03 14:28:22,520 - __main__ - INFO - üî® Construction du mod√®le LSTM\n",
      "2026-02-03 14:28:23,184 - __main__ - INFO - ‚úì Mod√®le construit - Param√®tres: 9,123\n",
      "2026-02-03 14:28:23,219 - __main__ - INFO - ‚úì Mod√®le compil√©\n",
      "2026-02-03 14:28:23,464 - __main__ - INFO - ‚úì Validation activ√©e\n",
      "2026-02-03 14:36:44,442 - __main__ - INFO - üìä Epoch   1/5 ‚îÇ Train: 26164.2207 ‚îÇ Val: 11498.8760 ‚îÇ LR: 9.99e-04 ‚îÇ ‚è± 501.0s\n",
      "2026-02-03 14:47:45,861 - __main__ - INFO - üìä Epoch   2/5 ‚îÇ Train: 22017.3809 ‚îÇ Val: 10619.5352 ‚îÇ LR: 9.99e-04 ‚îÇ ‚è± 661.4s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =================================================================================\n",
    "# SCRIPT PRINCIPAL\n",
    "# =================================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"üöÄ Deep Hedging GMAB - D√©marrage\")\n",
    "    \n",
    "    try:\n",
    "        # Chargement des donn√©es\n",
    "        logger.info(\"üìÇ Chargement des donn√©es...\")\n",
    "        data = np.load('market_scenarios_final.npz')\n",
    "        S_paths = data['S_paths'].astype(np.float32)\n",
    "        L_paths = data['L_paths'].astype(np.float32)\n",
    "        P_paths = data['P_paths'].astype(np.float32)\n",
    "        swaps_paths = data['swap_paths'].astype(np.float32)\n",
    "        logger.info(f\"‚úì Donn√©es charg√©es: {S_paths.shape[0]} sc√©narios\")\n",
    "        \n",
    "        # Param√®tres\n",
    "        N_contracts = 1\n",
    "        T_years = 25\n",
    "        N_per_year = 12\n",
    "        N_max = T_years * N_per_year\n",
    "        dt_float = 1.0 / N_per_year\n",
    "        \n",
    "        # S√©lection des ZC\n",
    "        maturities_to_use = [1, 5, 15, 20, 25]\n",
    "        indices_zc = [m - 1 for m in maturities_to_use]\n",
    "        P_selected = P_paths[:, :, indices_zc]\n",
    "        P_selected = roll_zc_paths(P_selected, maturities_to_use, N_per_year)\n",
    "        \n",
    "        # Taux court\n",
    "        T_grid = np.arange(1, 26)\n",
    "        r_paths = np.zeros((S_paths.shape[0], N_max + 1), dtype=np.float32)\n",
    "        for m in range(S_paths.shape[0]):\n",
    "            for k in range(N_max + 1):\n",
    "                t = k * dt_float\n",
    "                beta_idx = np.searchsorted(T_grid, t, side='right')\n",
    "                if beta_idx < len(T_grid):\n",
    "                    r_paths[m, k] = np.log(1 + dt_float * L_paths[m, k, beta_idx]) / dt_float\n",
    "                else:\n",
    "                    r_paths[m, k] = r_paths[m, k - 1] if k > 0 else 0.02\n",
    "        \n",
    "        # Passif GMAB\n",
    "        Z_paths = np.maximum.accumulate(S_paths, axis=1)\n",
    "        S_T = S_paths[:, -1:]\n",
    "        S_T_broadcast = np.tile(S_T, (1, N_max + 1))\n",
    "        P_Tn = P_paths[:, :, -1]\n",
    "        L_t = N_contracts * P_Tn * np.maximum(Z_paths, S_T_broadcast)\n",
    "        H_full = L_t[:, -1].astype(np.float32)\n",
    "        \n",
    "        # Pr√©paration des features\n",
    "        logger.info(\"üéØ Pr√©paration des features...\")\n",
    "        n_assets = 7 # 1 action + 5 ZC + 1 swap\n",
    "        n_features = 10\n",
    "        t_norm = np.linspace(0, 1, N_max + 1, dtype=np.float32)\n",
    "        \n",
    "        input_tensor = np.zeros((S_paths.shape[0], N_max + 1, n_features), dtype=np.float32)\n",
    "        for t in range(N_max + 1):\n",
    "            input_tensor[:, t, 0] = S_paths[:, t]\n",
    "            input_tensor[:, t, 1:6] = P_selected[:, t, :]\n",
    "            input_tensor[:, t, 6] = swaps_paths[:, t]\n",
    "            input_tensor[:, t, 7] = Z_paths[:, t]\n",
    "            input_tensor[:, t, 8] = r_paths[:, t]\n",
    "            input_tensor[:, t, 9] = t_norm[t]\n",
    "        \n",
    "        # Log-transformation\n",
    "        input_tensor[:, :, :6] = np.log(input_tensor[:, :, :6] + 1e-8)\n",
    "        \n",
    "        # Split des donn√©es\n",
    "        logger.info(\"‚úÇÔ∏è Division des donn√©es...\")\n",
    "        train_size, val_size, test_size = 10000, 2000, 2000\n",
    "        \n",
    "        indices = np.arange(S_paths.shape[0])\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_idx = indices[:train_size]\n",
    "        val_idx = indices[train_size:train_size + val_size]\n",
    "        test_idx = indices[train_size + val_size:train_size + val_size + test_size]\n",
    "        \n",
    "        train_input = input_tensor[train_idx]\n",
    "        valid_input = input_tensor[val_idx]\n",
    "        test_input = input_tensor[test_idx]\n",
    "        \n",
    "        H_train = H_full[train_idx]\n",
    "        H_valid = H_full[val_idx]\n",
    "        H_test = H_full[test_idx]\n",
    "        \n",
    "        r_train = r_paths[train_idx, :-1]\n",
    "        r_valid = r_paths[val_idx, :-1]\n",
    "        r_test = r_paths[test_idx, :-1]\n",
    "        \n",
    "        L_train = L_t[train_idx]\n",
    "        L_valid = L_t[val_idx]\n",
    "        L_test = L_t[test_idx]\n",
    "        \n",
    "        # Normalisation\n",
    "        logger.info(\"üìè Normalisation...\")\n",
    "        normalizer = DataNormalizer(method='standard')\n",
    "        train_input_norm = normalizer.fit_transform(train_input)\n",
    "        valid_input_norm = normalizer.transform(valid_input)\n",
    "        test_input_norm = normalizer.transform(test_input)\n",
    "        \n",
    "        # Configuration\n",
    "        logger.info(\"‚öôÔ∏è Configuration du mod√®le...\")\n",
    "        config = ModelConfig(\n",
    "            model_type=ModelType.LSTM,\n",
    "            n_assets=n_assets,\n",
    "            n_features=n_features + 1,\n",
    "            hidden_layers=2,\n",
    "            units=32,\n",
    "            learning_rate=0.001,\n",
    "            alpha_cvar=0.99,\n",
    "            lambda_cost=0.001,\n",
    "            scale_factor_zc=1.0,\n",
    "            dropout_rate=0.3,\n",
    "            recurrent_dropout=0.2,\n",
    "            batch_normalization=True\n",
    "        )\n",
    "        \n",
    "        engine = DeepHedgingEngine(config)\n",
    "        \n",
    "        V0_train = float(L_train[:, 0].mean())\n",
    "        V0_valid = float(L_valid[:, 0].mean())\n",
    "        V0_test = float(L_test[:, 0].mean())\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        history = engine.train(\n",
    "            train_input=train_input,\n",
    "            train_input_norm=train_input_norm,\n",
    "            H_train=H_train,\n",
    "            r_train=r_train,\n",
    "            V0_train=V0_train,\n",
    "            V0_valid=V0_valid,\n",
    "            dt=dt_float,\n",
    "            val_input=valid_input,\n",
    "            val_input_norm=valid_input_norm,\n",
    "            H_val=H_valid,\n",
    "            r_val=r_valid,\n",
    "            epochs=5,\n",
    "            batch_size=2048,\n",
    "            patience=10,\n",
    "            loss_fn=cvar_loss(alpha=0.99),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # √âvaluation\n",
    "        results,V_T, delta_test, V_trajectory,PnL_test = engine.evaluate(\n",
    "            test_input, test_input_norm, H_test, r_test, dt_float, V0_test\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        # Pr√©dictions d√©taill√©es\n",
    "        V_T, delta_test, V_trajectory = engine.predict(\n",
    "            test_input, test_input_norm, r_test, dt_float, V0_test, return_trajectory=True\n",
    "        )\n",
    "        \n",
    "        PnL_test = H_test - V_T\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Visualisations\n",
    "        logger.info(\"üé® G√©n√©ration des visualisations...\")\n",
    "        plot_training_history(history, save_path='training_history.png', show=True)\n",
    "        plot_detailed_results(PnL_test, delta_test, L_test, V_trajectory, \n",
    "                            T_years=T_years, save_path='detailed_results.png', show=True)\n",
    "        print_metrics_table(results)\n",
    "        \n",
    "        # Sauvegarde\n",
    "        logger.info(\"üíæ Sauvegarde...\")\n",
    "        engine.save('saved_model')\n",
    "        \n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"‚úÖ EX√âCUTION TERMIN√âE AVEC SUCC√àS\")\n",
    "        logger.info(f\"   CVaR 99%: {results['cvar_99']:.2f}\")\n",
    "        logger.info(f\"   Efficacit√©: {results['efficiency']:.2%}\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"‚ùå Fichier non trouv√©: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27e510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CVaR_H_test=cvar_loss(alpha=0.99)(None, tf.expand_dims(H_test, axis=1)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a210d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVaR 99% du passif sur le jeu de test: [25090.37]\n"
     ]
    }
   ],
   "source": [
    "print(f\"CVaR 99% du passif sur le jeu de test: {CVaR_H_test:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a90e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oliseh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
